{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizerFast, TFRobertaModel\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import _pickle as pickle\n",
    "import gc\n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "pd.set_option('display.max_rows', 300)\n",
    "\n",
    "\n",
    "def pmap(func, x, n_jobs = 32):\n",
    "    result = joblib.Parallel(n_jobs = n_jobs, prefer = 'processes', verbose = 0)(joblib.delayed(func)(args) for args in tqdm(x))\n",
    "    return result\n",
    "\n",
    "tf.config.list_physical_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b727f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata():\n",
    "    with open('arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
    "        for line in f:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5067c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n",
    "\n",
    "titles_tags_dict = {k:[] for k in keys}\n",
    "count = 0\n",
    "for paper in tqdm(metadata, total = 1700000):\n",
    "    parsed = json.loads(paper)\n",
    "    for k in keys:\n",
    "        titles_tags_dict[k].append(parsed[k])\n",
    "#     titles_tags_dict[\"title\"].append(parsed['title'])\n",
    "#     titles_tags_dict[\"tags\"].append(parsed['categories'])\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e1f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_news(start_year, end_year):\n",
    "#     df = []\n",
    "#     for i in tqdm(range(start_year, end_year)):\n",
    "#         for j in tqdm(range(1,13), leave = False):\n",
    "#             try:\n",
    "#                 df.append(pd.read_parquet(f'./news/{i}_{j}_news.parquet'))\n",
    "#             except:\n",
    "#                 pass\n",
    "#     return pd.concat(df).dropna(subset = [\"article\", \"title\"])\n",
    "\n",
    "# df_news = load_news(2016,2021)\n",
    "\n",
    "# print('Loaded')\n",
    "# df_news['len'] = pmap(lambda x : len(str(x).split()), df_news['article'].values, n_jobs = 32)\n",
    "\n",
    "# print('Len computed')\n",
    "\n",
    "# print(df_news.shape)\n",
    "# df_news.to_parquet('news_with_len.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "\n",
    "max_len = 64\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def random_crop(x, max_len = 256, min_len = 32):\n",
    "    x = str(x).split()\n",
    "    max_len = min(max_len, len(x))\n",
    "    min_len = min(min_len, max_len - 1)\n",
    "    \n",
    "    st = np.random.randint(0, max_len - min_len)\n",
    "    en = np.random.randint(min_len, max_len)\n",
    "    x = x[st:st+en]\n",
    "    x = \" \".join(x)\n",
    "    return x\n",
    "\n",
    "class BertEmbedding(tf.keras.Model):\n",
    "    def __init__(self, max_len = 128, encoder_weights = None):\n",
    "        super(BertEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.encoder = TFRobertaModel.from_pretrained('roberta-base')\n",
    "            \n",
    "        self.dense1 = tf.keras.layers.Dense(512, activation = 'relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(512, activation = 'linear')\n",
    "        \n",
    "        dummy_input = {\n",
    "            'input_ids' : np.random.randint(0,100,size = (8, self.max_len)).astype('int32'),\n",
    "            'attention_mask' : np.random.randint(0,2,size = (8, self.max_len)).astype('int32'),\n",
    "        }\n",
    "        dummy = self(dummy_input)\n",
    "        \n",
    "    \n",
    "    def call(self, x, training = True):\n",
    "        encoded = self.encoder(input_ids = x['input_ids'], \n",
    "                               training =  training, \n",
    "                               attention_mask = x['attention_mask'],\n",
    "                              )[0][:,0,:]\n",
    "\n",
    "        \n",
    "        embedding = self.dense1(encoded)\n",
    "        embedding = self.dense2(embedding)\n",
    "        return embedding\n",
    "            \n",
    "class SiameseBert(tf.keras.Model):\n",
    "    def __init__(self, max_len = 128, encoder_weights = None):\n",
    "        super(SiameseBert, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.encoder = BertEmbedding(max_len)\n",
    "        self.cosine = tf.keras.layers.Dot(axes = -1, normalize=True)\n",
    "        \n",
    "        if encoder_weights:\n",
    "            self.encoder.load_weights(encoder_weights)\n",
    "        \n",
    "        dummy_input = [{\n",
    "            'input_ids' : np.random.randint(0,100,size = (8, self.max_len)).astype('int32'),\n",
    "            'attention_mask' : np.random.randint(0,2,size = (8, self.max_len)).astype('int32'),\n",
    "        },\n",
    "        {\n",
    "            'input_ids' : np.random.randint(0,100,size = (8, self.max_len)).astype('int32'),\n",
    "            'attention_mask' : np.random.randint(0,2,size = (8, self.max_len)).astype('int32'),\n",
    "        }]\n",
    "        \n",
    "        dummy = self(dummy_input)\n",
    "        \n",
    "    def call(self, x, training = True):\n",
    "        \n",
    "        encoded_a = self.encoder(x[0])\n",
    "        encoded_b = self.encoder(x[1])\n",
    "        \n",
    "        cosine_sim = self.cosine([encoded_a, encoded_b])\n",
    "        cosine_sim = (cosine_sim + 1.001)/2.002\n",
    "        \n",
    "        return cosine_sim\n",
    "\n",
    "class DataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, articles, batch_size=  32, max_len = 256):\n",
    "        self.corpus = articles\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.n_sample = len(self.corpus)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000000\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        ## Sampling data\n",
    "        idx = np.random.randint(0, self.n_sample -1, size = self.batch_size)\n",
    "        sample = self.corpus[idx]\n",
    "        \n",
    "        ## Creating Target Array with half zero then half 1\n",
    "        y = np.zeros(self.batch_size)\n",
    "        b = self.batch_size // 2\n",
    "        y[b:] += 1\n",
    "        \n",
    "        idx_dum = np.random.randint(0, self.n_sample -1, size = b)\n",
    "        sample_dum = self.corpus[idx_dum]\n",
    "        \n",
    "        ## Creating two random augmentation of each sample articles\n",
    "        p1 = np.array([random_crop(elt, max_len = self.max_len) for elt in sample])\n",
    "        p2 = np.array([random_crop(elt, max_len = self.max_len) for elt in sample])\n",
    "        p3 = np.array([random_crop(elt, max_len = self.max_len) for elt in sample_dum])\n",
    "        \n",
    "        ## Shuffling the first half of the second augmentation to have different article on average (this also creates some negative example by default)\n",
    "        shuffle_id = np.array(list(range(b)))\n",
    "        np.random.shuffle(shuffle_id) \n",
    "        \n",
    "        p2[np.array(list(range(b)))] = p3\n",
    "        \n",
    "        ## Shuffling the whole batch\n",
    "        shuffle_id = np.array(list(range(self.batch_size)))\n",
    "        np.random.shuffle(shuffle_id) \n",
    "        p1 = p1[shuffle_id]\n",
    "        p2 = p2[shuffle_id]\n",
    "        y = y[shuffle_id]\n",
    "        \n",
    "        ## Tokenization\n",
    "        p1 = tokenizer.batch_encode_plus(list(p1),add_special_tokens = True, max_length = self.max_len, truncation = True, return_tensors = 'np', padding  = 'max_length')\n",
    "        p2 = tokenizer.batch_encode_plus(list(p2),add_special_tokens = True, max_length = self.max_len, truncation = True, return_tensors = 'np', padding  = 'max_length')\n",
    "        p1 = {elt:p1[elt].astype('int32') for elt in p1}\n",
    "        p2 = {elt:p2[elt].astype('int32') for elt in p2}\n",
    "        \n",
    "        ## Model inputs creation\n",
    "        inputs_a = {\n",
    "            'input_ids': p1['input_ids'],\n",
    "            'attention_mask': p1['attention_mask']}\n",
    "        inputs_b = {\n",
    "            'input_ids':  p2['input_ids'],\n",
    "            'attention_mask': p2['attention_mask']\n",
    "        }\n",
    "        inputs = inputs_a, inputs_b\n",
    "        return inputs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cf28cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('data.pickle', 'rb') as f:\n",
    "    df_news = pickle.load(f)\n",
    "    \n",
    "df_news['article'] = df_news['title'] + ' </s> ' + df_news['abstract'] \n",
    "df_news['len'] = df_news['article'].apply(lambda x : len(str(x).split())*1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = df_news[df_news['len']>= 2 * max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.reset_index(inplace = True, drop = True)\n",
    "idx = np.random.randint(0, df_news.shape[0], size = min(df_news.shape[0], 100000))\n",
    "X = df_news['article'].iloc[idx].values\n",
    "print(X.shape)\n",
    "X_train, X_test, _, _ = train_test_split(X, np.zeros(len(X)), test_size=0.2, random_state=42)\n",
    "\n",
    "# del df_news, df_news_bis\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa47950",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1571963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseBert(max_len = max_len)#, encoder_weights = './models/siamese_encoder_256.tf')\n",
    "\n",
    "model.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = tf.keras.optimizers.Adam(3e-5, 1e-8),\n",
    "    metrics = ['accuracy', 'AUC']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b8671",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_gen = DataGen(X_train, batch_size = batch_size, max_len = max_len)\n",
    "test_gen = DataGen(X_test, batch_size = 4096, max_len = max_len)\n",
    "x_test,  y_test = test_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b91604",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "x,y = train_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957244f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_auc', min_delta=0.0001, patience=7, verbose=1, \n",
    "                                                mode='max', restore_best_weights=True)\n",
    "reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.1, patience=3, verbose=1, \n",
    "                                                     mode='max', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "\n",
    "validation_batch_size = 4*batch_size\n",
    "n_epochs = 10\n",
    "steps_per_epoch = int(50000/batch_size) + 1\n",
    "\n",
    "model.fit(train_gen, \n",
    "          epochs=n_epochs,\n",
    "          steps_per_epoch = steps_per_epoch,       \n",
    "          validation_data=(x_test,  y_test),       \n",
    "          validation_batch_size = validation_batch_size,     \n",
    "          callbacks = [early, reduce],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ecf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.save_weights('./models/sci-siamese_encoder_64.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25018c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoder.load_weights('./models/siamese_encoder_128.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bcdccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99cca07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip to instantiate a model of type clip_text_model. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip to instantiate a model of type clip_vision_model. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x206f0d66850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import CLIPProcessor, CLIPTokenizer, CLIPTextConfig, CLIPVisionConfig\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "import os\n",
    "from transformers.models.clip.modeling_tf_clip import (\n",
    "    TFCLIPTextMainLayer, \n",
    "    TFCLIPTextTransformer,\n",
    "    TFCLIPMainLayer,\n",
    "    TFCLIPModel,\n",
    "    TFCLIPVisionMainLayer,\n",
    "    TFCLIPVisionTransformer\n",
    ")\n",
    "\n",
    "class ClipTextEmbedder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ClipTextEmbedder, self).__init__()\n",
    "        self.config=CLIPTextConfig.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.encoder=TFCLIPTextTransformer(self.config)\n",
    "        self.projection = tf.keras.layers.Dense(\n",
    "            units=self.config.projection_dim,\n",
    "#             kernel_initializer=get_initializer(self.config.text_config['hidden_size']**-0.5 * self.config.initializer_factor),\n",
    "            use_bias=False,\n",
    "            name=\"text_projection\",\n",
    "        )\n",
    "        # if you want to instanciate it from huggingface base model; useful to save initial weights locally the first time\n",
    "#         self.encoder=TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").clip.text_model\n",
    "#         self.projection=TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").clip.text_projection \n",
    "        \n",
    "        \n",
    "    def call(self, x, training = True):\n",
    "        output=self.encoder(input_ids=x['input_ids'],\n",
    "                            attention_mask=x['attention_mask'],\n",
    "                            position_ids=None,\n",
    "                            output_attentions=False,\n",
    "                            output_hidden_states=False,\n",
    "                            return_dict=False,\n",
    "                            training=training\n",
    "                           )\n",
    "        output=output[1]\n",
    "        output=self.projection(inputs=output)\n",
    "        output=output / tf.norm(tensor=output, ord=\"euclidean\", axis=-1, keepdims=True)\n",
    "        return output\n",
    "    \n",
    "class ClipImageEmbedder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ClipImageEmbedder, self).__init__()\n",
    "        self.config=CLIPVisionConfig.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.encoder=TFCLIPVisionTransformer(self.config)\n",
    "        self.projection = tf.keras.layers.Dense(\n",
    "            units=self.config.projection_dim,\n",
    "#             kernel_initializer=get_initializer(self.config.text_config['hidden_size']**-0.5 * self.config.initializer_factor),\n",
    "            use_bias=False,\n",
    "            name=\"visual_projection\",\n",
    "        )\n",
    "        # if you want to instanciate it from huggingface base model; useful to save initial weights locally the first time\n",
    "#         self.encoder=TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").clip.vision_model \n",
    "#         self.projection=TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").clip.visual_projection  \n",
    "        \n",
    "        \n",
    "    def call(self, x, training = True):\n",
    "        output=self.encoder(pixel_values=x['pixel_values'],\n",
    "                            output_attentions=False,\n",
    "                            output_hidden_states=False,\n",
    "                            return_dict=False,\n",
    "                            training=training                           \n",
    "                           )\n",
    "        output=output[1]\n",
    "        output=self.projection(inputs=output)\n",
    "        output=output / tf.norm(tensor=output, ord=\"euclidean\", axis=-1, keepdims=True)\n",
    "        return output\n",
    "\n",
    "def tokenize_texts(texts, max_length=64):\n",
    "    tokenizer=CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    txt_inputs = tokenizer.batch_encode_plus(texts, \n",
    "                                     padding='max_length', \n",
    "                                     return_tensors=\"np\",\n",
    "                                     max_length=64)\n",
    "    txt_inputs = {elt:np.array(txt_inputs[elt]).astype('int32') for elt in txt_inputs}\n",
    "    return txt_inputs\n",
    "\n",
    "def tokenize_imgs(images):\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    img_inputs=processor(images=images, return_tensors=\"np\")\n",
    "    img_inputs={elt:np.array(img_inputs[elt]) for elt in img_inputs}\n",
    "    return img_inputs\n",
    "\n",
    "def compute_sim(image_embeddings, text_embeddings):\n",
    "    logits_per_text = tf.matmul(text_embeds, image_embeds, transpose_b=True)* tf.math.exp(4.6)\n",
    "    logits_per_image = tf.transpose(logits_per_text)\n",
    "    logits_per_image = tf.nn.softmax(logits_per_image,axis=1)\n",
    "    return logits_per_image\n",
    "    \n",
    "model_text = ClipTextEmbedder()\n",
    "model_text.load_weights('clip-base/clip_text_embedding/weights')\n",
    "# model_text.save_weights('clip-base/clip_text_embedding/weights')\n",
    "\n",
    "model_image = ClipImageEmbedder()\n",
    "model_image.load_weights('clip-base/clip_image_embedding/weights')\n",
    "# model_image.save_weights('clip-base/clip_image_embedding/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "534bf419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip to instantiate a model of type clip_text_model. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip to instantiate a model of type clip_vision_model. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "class TFClip(tf.keras.Model):\n",
    "    def __init__(self, save_folder=None):\n",
    "        super(TFClip, self).__init__()\n",
    "        self.text_encoder=ClipTextEmbedder()\n",
    "        self.image_encoder=ClipImageEmbedder()\n",
    "        \n",
    "        if save_folder:\n",
    "            self.text_encoder.load_weights(os.path.join(save_folder,'clip_text_embedding','weights'))\n",
    "            self.image_encoder.load_weights(os.path.join(save_folder,'clip_image_embedding','weights'))\n",
    "        \n",
    "    def call(self, x, training=True):\n",
    "        [text_inputs, image_inputs] = x\n",
    "        text_embedding=self.text_encoder(text_inputs,training=training)\n",
    "        image_embedding=self.image_encoder(image_inputs,training=training)\n",
    "        cosine=compute_sim(image_embedding,text_embedding)\n",
    "        return cosine\n",
    "    \n",
    "clip = TFClip('clip-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2859d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "txts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "text_inputs=tokenize_texts(txts)\n",
    "\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "imgs=[image]\n",
    "image_inputs = tokenize_imgs(imgs)\n",
    "\n",
    "inputs=[text_inputs, image_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9a53ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n"
     ]
    }
   ],
   "source": [
    "text_embeds=model_text.predict(txt_inputs, batch_size=2,verbose=True)\n",
    "image_embeds=model_image.predict(image_inputs, batch_size=2,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04dd83d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.99471927, 0.00528076]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cfcfb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[24.442019, 19.203629]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_per_text = tf.matmul(text_embeds, image_embeds, transpose_b=True)* tf.math.exp(4.6)\n",
    "logits_per_image = tf.transpose(logits_per_text)\n",
    "logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_sim(image_embeds, text_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f601cc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.99471927, 0.00528076]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a0a634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cbe6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
